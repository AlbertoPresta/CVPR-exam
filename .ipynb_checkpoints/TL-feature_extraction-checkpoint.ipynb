{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE EXTRACTION \n",
    "\n",
    "In this script I will complete the last point of the third part of the project.\n",
    "\n",
    "employ the pre-trained network as a feature extractor, accessing the activation of an intermediate layer (for instance the last convolutional layer) and train a multiclass linear SVM. For implementing the multiclass SVM use any of the approaches seen in the lectures, for instance DAG.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL DEDICATED TO IMPORT PACKAGES USEFULL FOR OUR PURPOSES\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import os\n",
    "import cv2\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import History\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.utils import plot_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.layers import Conv2D,Flatten,Dropout, MaxPooling2D,AveragePooling2D\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "import import_ipynb\n",
    "import utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATION OF TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOME IMPORTANT GLOBAL VARIABLE\n",
    "labels = [\"Bedroom\",\"Coast\",\"Forest\",\"HighWay\",\"Industrial\",\"InsideCity\",\"Kitchen\",\"LivingRoom\",\"Mountain\",\"Office\",\"OpenCountry\",\"Store\",\"Street\",\"Suburb\",\"TallBuilding\"]\n",
    "train_dir = '../images/train/'\n",
    "test_dir = '../images/test/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_images = utils.list_of_path(labels,train_dir,shuf=False)\n",
    "train_data,train_labels = utils.read_and_process_images(list_of_images,dimension=224)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../images/train/Kitchen/image_0146.jpg'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_images[600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_images =  []\n",
    "\n",
    "\n",
    "#solo 10 per ogni classe--> altrimenti conto lunghissimo \n",
    "for i in range(10):\n",
    "    temp_img_K = image.load_img(list_of_images[600 + i],target_size=(224,224))\n",
    "    temp_img_K=image.img_to_array(temp_img_K)\n",
    "    train_images.append(temp_img_K)\n",
    "    \n",
    "    temp_img_Liv = image.load_img(list_of_images[700 + i],target_size=(224,224))\n",
    "    temp_img_Liv=image.img_to_array(temp_img_Liv)\n",
    "    train_images.append(temp_img_Liv)\n",
    "    \n",
    "    temp_img_Mount = image.load_img(list_of_images[800 + i],target_size=(224,224))\n",
    "    temp_img_Mount=image.img_to_array(temp_img_Mount)\n",
    "    train_images.append(temp_img_Mount)\n",
    "    \n",
    "    temp_img_Off = image.load_img(list_of_images[900 + i],target_size=(224,224))\n",
    "    temp_img_Off=image.img_to_array(temp_img_Off)\n",
    "    train_images.append(temp_img_Off)\n",
    "    \n",
    "    temp_img_Open = image.load_img(list_of_images[1000 + i],target_size=(224,224))\n",
    "    temp_img_Open=image.img_to_array(temp_img_Open)\n",
    "    train_images.append(temp_img_Open)\n",
    "    \n",
    "    temp_img_Street = image.load_img(list_of_images[1100 + i],target_size=(224,224))\n",
    "    temp_img_Street=image.img_to_array(temp_img_Street)\n",
    "    train_images.append(temp_img_Street)\n",
    "    \n",
    "    temp_img_Suburb = image.load_img(list_of_images[1200 + i],target_size=(224,224))\n",
    "    temp_img_Suburb=image.img_to_array(temp_img_Suburb)\n",
    "    train_images.append(temp_img_Suburb)\n",
    "    \n",
    "    \n",
    "    \n",
    "    temp_img_Tall = image.load_img(list_of_images[1300 + i],target_size=(224,224))\n",
    "    temp_img_Tall=image.img_to_array(temp_img_Tall)\n",
    "    train_images.append(temp_img_Tall)\n",
    "\n",
    "    \n",
    "    temp_img_Store = image.load_img(list_of_images[1400 + i],target_size=(224,224))\n",
    "    temp_img_Store=image.img_to_array(temp_img_Store)\n",
    "    train_images.append(temp_img_Store)\n",
    "    \n",
    "    temp_img_bed = image.load_img(list_of_images[i],target_size=(224,224))\n",
    "    temp_img_bed=image.img_to_array(temp_img_bed)\n",
    "    train_images.append(temp_img_bed)\n",
    "    \n",
    "    temp_img_coast = image.load_img(list_of_images[100 + i],target_size=(224,224))\n",
    "    temp_img_coast=image.img_to_array(temp_img_coast)\n",
    "    train_images.append(temp_img_coast)\n",
    "    \n",
    "    temp_img_For = image.load_img(list_of_images[200 + i],target_size=(224,224))\n",
    "    temp_img_For=image.img_to_array(temp_img_For)\n",
    "    train_images.append(temp_img_For)\n",
    "    \n",
    "    \n",
    "    temp_img_High = image.load_img(list_of_images[300 + i],target_size=(224,224))\n",
    "    temp_img_High=image.img_to_array(temp_img_High)\n",
    "    train_images.append(temp_img_High)\n",
    "    \n",
    "    \n",
    "    temp_img_Industrial = image.load_img(list_of_images[400 + i],target_size=(224,224))\n",
    "    temp_img_Industrial=image.img_to_array(temp_img_High)\n",
    "    train_images.append(temp_img_Industrial)\n",
    "    \n",
    "    temp_img_Inside = image.load_img(list_of_images[500 + i],target_size=(224,224))\n",
    "    temp_img_Inside=image.img_to_array(temp_img_Inside)\n",
    "    train_images.append(temp_img_Inside)\n",
    "\n",
    "    \n",
    "train_images=np.array(train_images)\n",
    "train_img=preprocess_input(train_images)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "train_labels = le.fit_transform(train_labels)\n",
    "train_labels_dummy=to_categorical(train_labels,15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_feature = []\n",
    "for i in range(150):\n",
    "    temp = model.predict(train_img[:i,:,:,:])\n",
    "    vgg16_feature.append(temp)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "f,ax = plt.subplots(1,5)\n",
    "for i in range(5):\n",
    "    ax[i].imshow(vgg16_feature[1][0,:,:,i])\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, my idea is to rectify each feature in a way to have vector instead of matrix.\n",
    "\n",
    "How big will we a feature after flattening? we have a 7X7 matrix, so feature vector will be 49-dimension.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_matrix = []\n",
    "feature_labels = []\n",
    "for i in range(1,150):\n",
    "    temporal = []\n",
    "    for j in range(512):\n",
    "        temp = np.asarray(vgg16_feature[i][0,:,:,j])\n",
    "        temp = np.reshape(temp,49)\n",
    "        temporal.append(temp)\n",
    "    temporal = np.reshape(temporal,49*512)\n",
    "    features_matrix.append(temporal)\n",
    "    if(0<=i<10):\n",
    "        feature_labels.append(0)\n",
    "    if (10<=i<20):\n",
    "        feature_labels.append(1)\n",
    "    if(20<=i<30):\n",
    "        feature_labels.append(2)\n",
    "    if(30<=i<40):\n",
    "        feature_labels.append(3)\n",
    "    if(40<=i<50):\n",
    "        feature_labels.append(4)\n",
    "    if(50<=i<60):\n",
    "        feature_labels.append(5)\n",
    "    if(60<=i<70):\n",
    "        feature_labels.append(6)\n",
    "    if(70<=i<80):\n",
    "        feature_labels.append(7)\n",
    "    if(80<=i<90):\n",
    "        feature_labels.append(8)\n",
    "    if(90<=i<100):\n",
    "        feature_labels.append(9)\n",
    "    if(100<=i<110):\n",
    "        feature_labels.append(10)\n",
    "    if(110<=i<120):\n",
    "        feature_labels.append(11)\n",
    "    if(120<=i<130):\n",
    "        feature_labels.append(12)\n",
    "    if(130<=i<140):\n",
    "        feature_labels.append(13)\n",
    "    if(140<=i<150):\n",
    "        feature_labels.append(14)\n",
    "\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76288"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-against-rest, eith winning-class aprroach (not DAG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_SVM(features_mat,feature_lab,num_classes = 15):\n",
    "    k = len(features_mat) # number of features map \n",
    "    classif = [SVC(kernel=\"linear\") for _ in range(num_classes)] # array of classifier to be trained\n",
    "    # one-vs-all approach with dag \n",
    "    curr_label = 0\n",
    "    for clf in classif:\n",
    "        print(curr_label)\n",
    "        clf = clf.fit(features_mat, np.array([1 if label==curr_label else -1 for label in feature_lab]))\n",
    "        curr_label = curr_label + 1\n",
    "    return classif\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = linear_SVM(features_matrix,feature_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATING TEST DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_images = utils.list_of_path(labels,test_dir,shuf=False)\n",
    "test_data,test_labels = utils.read_and_process_images(list_of_images,dimension=224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images =  []\n",
    "\n",
    "\n",
    "#solo 10 per ogni classe--> altrimenti conto lunghissimo \n",
    "for i in range(10):\n",
    "    temp_img_K = image.load_img(list_of_images[600 + i],target_size=(224,224))\n",
    "    temp_img_K=image.img_to_array(temp_img_K)\n",
    "    test_images.append(temp_img_K)\n",
    "    \n",
    "    temp_img_Liv = image.load_img(list_of_images[700 + i],target_size=(224,224))\n",
    "    temp_img_Liv=image.img_to_array(temp_img_Liv)\n",
    "    test_images.append(temp_img_Liv)\n",
    "    \n",
    "    temp_img_Mount = image.load_img(list_of_images[800 + i],target_size=(224,224))\n",
    "    temp_img_Mount=image.img_to_array(temp_img_Mount)\n",
    "    test_images.append(temp_img_Mount)\n",
    "    \n",
    "    temp_img_Off = image.load_img(list_of_images[900 + i],target_size=(224,224))\n",
    "    temp_img_Off=image.img_to_array(temp_img_Off)\n",
    "    test_images.append(temp_img_Off)\n",
    "    \n",
    "    temp_img_Open = image.load_img(list_of_images[1000 + i],target_size=(224,224))\n",
    "    temp_img_Open=image.img_to_array(temp_img_Open)\n",
    "    test_images.append(temp_img_Open)\n",
    "    \n",
    "    temp_img_Street = image.load_img(list_of_images[1100 + i],target_size=(224,224))\n",
    "    temp_img_Street=image.img_to_array(temp_img_Street)\n",
    "    test_images.append(temp_img_Street)\n",
    "    \n",
    "    temp_img_Suburb = image.load_img(list_of_images[1200 + i],target_size=(224,224))\n",
    "    temp_img_Suburb=image.img_to_array(temp_img_Suburb)\n",
    "    test_images.append(temp_img_Suburb)\n",
    "    \n",
    "    \n",
    "    \n",
    "    temp_img_Tall = image.load_img(list_of_images[1300 + i],target_size=(224,224))\n",
    "    temp_img_Tall=image.img_to_array(temp_img_Tall)\n",
    "    test_images.append(temp_img_Tall)\n",
    "\n",
    "    \n",
    "    temp_img_Store = image.load_img(list_of_images[1400 + i],target_size=(224,224))\n",
    "    temp_img_Store=image.img_to_array(temp_img_Store)\n",
    "    test_images.append(temp_img_Store)\n",
    "    \n",
    "    temp_img_bed = image.load_img(list_of_images[i],target_size=(224,224))\n",
    "    temp_img_bed=image.img_to_array(temp_img_bed)\n",
    "    test_images.append(temp_img_bed)\n",
    "    \n",
    "    temp_img_coast = image.load_img(list_of_images[100 + i],target_size=(224,224))\n",
    "    temp_img_coast=image.img_to_array(temp_img_coast)\n",
    "    test_images.append(temp_img_coast)\n",
    "    \n",
    "    temp_img_For = image.load_img(list_of_images[200 + i],target_size=(224,224))\n",
    "    temp_img_For=image.img_to_array(temp_img_For)\n",
    "    test_images.append(temp_img_For)\n",
    "    \n",
    "    \n",
    "    temp_img_High = image.load_img(list_of_images[300 + i],target_size=(224,224))\n",
    "    temp_img_High=image.img_to_array(temp_img_High)\n",
    "    test_images.append(temp_img_High)\n",
    "    \n",
    "    \n",
    "    temp_img_Industrial = image.load_img(list_of_images[400 + i],target_size=(224,224))\n",
    "    temp_img_Industrial=image.img_to_array(temp_img_High)\n",
    "    test_images.append(temp_img_Industrial)\n",
    "    \n",
    "    temp_img_Inside = image.load_img(list_of_images[500 + i],target_size=(224,224))\n",
    "    temp_img_Inside=image.img_to_array(temp_img_Inside)\n",
    "    test_images.append(temp_img_Inside)\n",
    "\n",
    "    \n",
    "test_images=np.array(test_images)\n",
    "test_img=preprocess_input(test_images)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "test_labels = le.fit_transform(test_labels)\n",
    "test_labels_dummy=to_categorical(test_labels,15)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_feature_test = []\n",
    "for i in range(150):\n",
    "    temp = model.predict(test_img[:i,:,:,:])\n",
    "    vgg16_feature_test.append(temp)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
