{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRANSFER LEARNING\n",
    "\n",
    "So far I achieved a test accuracy of about  65-66%, using only shallow network that I developed from scratch using keras.\n",
    "\n",
    "Now, The aim of this framework is to exploit pre-trained network (such as GVV or alexnet) and apply the so called \"transfer learning\".\n",
    "\n",
    "In the first part, I simply freeze all the weights of the network, but the last one dense layer and I train only the last weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from utils.ipynb\n"
     ]
    }
   ],
   "source": [
    "#CELL DEDICATED TO IMPORT PACKAGES USEFULL FOR OUR PURPOSES\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import os\n",
    "import cv2\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import History\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.utils import plot_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import Conv2D,Flatten,Dropout, MaxPooling2D,AveragePooling2D\n",
    "\n",
    "import import_ipynb\n",
    "import utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG \n",
    "\n",
    "The first pre-trained network that I will exploit is the VGG\n",
    "\n",
    "### PROBLEM OF DATA\n",
    "\n",
    "The first problem is that pre-trained cnn are yet trained, in particulare they are all trained for working with coloured images...but our data-set is formed by b&w images! How can I do? my simple idea is to copy the first Chanel values to other two channel and create a 3 channel image out of your gray scale image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOME IMPORTANT GLOBAL VARIABLE\n",
    "labels = [\"Bedroom\",\"Coast\",\"Forest\",\"HighWay\",\"Industrial\",\"InsideCity\",\"Kitchen\",\"LivingRoom\",\"Mountain\",\"Office\",\"OpenCountry\",\"Store\",\"Street\",\"Suburb\",\"TallBuilding\"]\n",
    "train_dir = '../images/train/'\n",
    "test_dir = '../images/test/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-processing of the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATION OF THE INITIAL DATA\n",
    "#train data\n",
    "list_of_images = utils.list_of_path(labels,train_dir)\n",
    "train_data,train_labels= utils.read_and_process_images(list_of_images,dimension=224)\n",
    "\n",
    "#test data\n",
    "list_of_images_test = utils.list_of_path(labels,test_dir)\n",
    "test_data,test_labels = utils.read_and_process_images(list_of_images_test,dimension=224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 224, 224)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA AUGMENTATION AND CREATION OF DUMMY VARIABLE \n",
    "#Â augmented train data with left-to-right reflection and cropping-->\n",
    "train_data_aug,train_labels_aug = utils.data_augmentation(train_data,train_labels)\n",
    "test_data_aug, test_labels_aug = utils.data_augmentation(test_data,test_labels)\n",
    "\n",
    "\n",
    "# create one-note-encoding to make labels feasible for the cnn\n",
    "test_labels_dummy = to_categorical(test_labels_aug,15)\n",
    "train_labels_dummy = to_categorical(train_labels_aug,15)\n",
    "test_labels_dummy = to_categorical(test_labels_aug,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 224, 224)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_aug.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 224, 224)\n",
      "(3000, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# AUGMENT OF ONE THE DIMENSION \n",
    "# reshape the data in order to make them feasible \n",
    "print(train_data_aug.shape)  # (64, 224, 224)\n",
    "rgb_batch = np.repeat(train_data_aug[..., np.newaxis], 3, -1)\n",
    "print(rgb_batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5970, 224, 224)\n",
      "(5970, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print(test_data_aug.shape)  # (64, 224, 224)\n",
    "test_batch = np.repeat(test_data_aug[..., np.newaxis], 3, -1)\n",
    "print(test_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg16_model(img_rows, img_cols, channel=1, num_classes=None):\n",
    "    adam = optimizers.Adam()\n",
    "\n",
    "    model = VGG16(weights='imagenet', include_top=True)\n",
    "\n",
    "    model.layers.pop()\n",
    "    x=Dense(num_classes, activation='softmax')(model.output)\n",
    "\n",
    "    model=Model(model.input,x)\n",
    "\n",
    "\n",
    "\n",
    "    for layer in model.layers[:-1]:\n",
    "        print(layer.output)\n",
    "\n",
    "        layer.trainable = False\n",
    "\n",
    "\n",
    "    \n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_2:0\", shape=(None, 224, 224, 3), dtype=float32)\n",
      "Tensor(\"block1_conv1_1/Relu:0\", shape=(None, 224, 224, 64), dtype=float32)\n",
      "Tensor(\"block1_conv2_1/Relu:0\", shape=(None, 224, 224, 64), dtype=float32)\n",
      "Tensor(\"block1_pool_1/MaxPool:0\", shape=(None, 112, 112, 64), dtype=float32)\n",
      "Tensor(\"block2_conv1_1/Relu:0\", shape=(None, 112, 112, 128), dtype=float32)\n",
      "Tensor(\"block2_conv2_1/Relu:0\", shape=(None, 112, 112, 128), dtype=float32)\n",
      "Tensor(\"block2_pool_1/MaxPool:0\", shape=(None, 56, 56, 128), dtype=float32)\n",
      "Tensor(\"block3_conv1_1/Relu:0\", shape=(None, 56, 56, 256), dtype=float32)\n",
      "Tensor(\"block3_conv2_1/Relu:0\", shape=(None, 56, 56, 256), dtype=float32)\n",
      "Tensor(\"block3_conv3_1/Relu:0\", shape=(None, 56, 56, 256), dtype=float32)\n",
      "Tensor(\"block3_pool_1/MaxPool:0\", shape=(None, 28, 28, 256), dtype=float32)\n",
      "Tensor(\"block4_conv1_1/Relu:0\", shape=(None, 28, 28, 512), dtype=float32)\n",
      "Tensor(\"block4_conv2_1/Relu:0\", shape=(None, 28, 28, 512), dtype=float32)\n",
      "Tensor(\"block4_conv3_1/Relu:0\", shape=(None, 28, 28, 512), dtype=float32)\n",
      "Tensor(\"block4_pool_1/MaxPool:0\", shape=(None, 14, 14, 512), dtype=float32)\n",
      "Tensor(\"block5_conv1_1/Relu:0\", shape=(None, 14, 14, 512), dtype=float32)\n",
      "Tensor(\"block5_conv2_1/Relu:0\", shape=(None, 14, 14, 512), dtype=float32)\n",
      "Tensor(\"block5_conv3_1/Relu:0\", shape=(None, 14, 14, 512), dtype=float32)\n",
      "Tensor(\"block5_pool_1/MaxPool:0\", shape=(None, 7, 7, 512), dtype=float32)\n",
      "Tensor(\"flatten_1/Reshape:0\", shape=(None, None), dtype=float32)\n",
      "Tensor(\"fc1_1/Relu:0\", shape=(None, 4096), dtype=float32)\n",
      "Tensor(\"fc2_1/Relu:0\", shape=(None, 4096), dtype=float32)\n",
      "Tensor(\"predictions_1/Softmax:0\", shape=(None, 1000), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "mod = vgg16_model(224,224,3,15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 15)                15015     \n",
      "=================================================================\n",
      "Total params: 138,372,559\n",
      "Trainable params: 15,015\n",
      "Non-trainable params: 138,357,544\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mod.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2550 samples, validate on 450 samples\n",
      "Epoch 1/10\n",
      "  64/2550 [..............................] - ETA: 17:41 - loss: 2.7107 - accuracy: 0.0156    "
     ]
    }
   ],
   "source": [
    "history = History()\n",
    "earlyStopping = EarlyStopping(min_delta=0.10,patience = 10)\n",
    "mod.fit(rgb_batch,train_labels_dummy,batch_size=32,epochs=10,validation_split=0.15,shuffle=True,callbacks=[earlyStopping,history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3352f3bdab80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_aug\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_labels_dummy_aug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'm' is not defined"
     ]
    }
   ],
   "source": [
    "m.evaluate(test_data_aug,test_labels_dummy_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRASPORTO QUA LA PARTE UTILS PERCHE' SERVE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import os\n",
    "import cv2\n",
    "from random import shuffle\n",
    "import itertools\n",
    "\n",
    "\n",
    "\n",
    "def read_and_process_images(list_of_images,dimension=64):\n",
    "    x = [] #array of images\n",
    "    y = [] #array of labels\n",
    "    for image in list_of_images:\n",
    "        x.append(cv2.resize(cv2.imread(image,cv2.IMREAD_GRAYSCALE),(dimension,dimension),interpolation=cv2.INTER_CUBIC))\n",
    "        if 'Bedroom' in image:\n",
    "            y.append(0)\n",
    "        if 'Coast' in image:\n",
    "            y.append(1)\n",
    "        if 'Forest' in image:\n",
    "            y.append(2)\n",
    "        if 'HighWay' in image:\n",
    "            y.append(3)\n",
    "        if 'Industrial' in image:\n",
    "            y.append(4)\n",
    "        if 'InsideCity' in image:\n",
    "            y.append(5)\n",
    "        if 'Kitchen' in image:\n",
    "            y.append(6)\n",
    "        if 'LivingRoom' in image:\n",
    "            y.append(7)\n",
    "        if 'Mountain' in image:\n",
    "            y.append(8)\n",
    "        if 'Office' in image:\n",
    "            y.append(9)\n",
    "        if 'OpenCountry' in image:\n",
    "            y.append(10)\n",
    "        if 'Store' in image:\n",
    "            y.append(11)\n",
    "        if 'Street' in image:\n",
    "            y.append(12)\n",
    "        if 'Suburb' in image:\n",
    "            y.append(13)\n",
    "        if 'TallBuilding' in image:\n",
    "            y.append(14)\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    x = x/255\n",
    "    return x,y\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def list_of_path(lab,path):\n",
    "    x = []\n",
    "    for i in lab:\n",
    "        s = path+i+'/{}'\n",
    "        temp = [s.format(i) for i in os.listdir(path+i+'/')]\n",
    "        x = x + temp\n",
    "    x = np.random.permutation(x) \n",
    "    return x\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,string,directory,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if(i==j):\n",
    "            plt.text(j, i, format(cm[i, j], fmt),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(directory +'confusion_matrix'+string+'.jpg')\n",
    "    \n",
    "    \n",
    "    \n",
    "def data_augmentation(list_of_images,list_of_labels, cropping = False,start_x = 0,start_y = 0, crop_x = 0,crop_y = 0,dimension=64):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(len(list_of_images)):\n",
    "        x.append(list_of_images[i])\n",
    "        y.append(list_of_labels[i])\n",
    "    #left-to-right part\n",
    "    for i in range(len(list_of_images)):\n",
    "        x.append(cv2.flip(list_of_images[i],1))\n",
    "        y.append(list_of_labels[i])\n",
    "    #eventually cropping part\n",
    "    if (cropping==True):\n",
    "        for i in range(len(list_of_images)): # for now I impose \n",
    "            dim_y = list_of_images.shape[0] - crop_y \n",
    "            dim_x = list_of_images.shape[1] - crop_x\n",
    "            image = list_of_images[i]\n",
    "            temp = image[start_y:start_y+dim_y,start_x:start_x+dim_x]\n",
    "            x.append(cv2.resize(temp,(dimension,dimension),interpolation=cv2.INTER_CUBIC))\n",
    "            y.append(list_of_labels[i])\n",
    "            \n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
