{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OVERVIEW\n",
    "\n",
    "In this second part, as I just said, I will try to improve my results by building different cnn with differet characteristics. As I said before, my main goal is to improve test accuracy, first of all, I perform  data augmentation in order to have more data to train my network; we will see that just doing this stuff improves a lot our results. the details of data augmentation are described in the pdf and in the function dedicated to data aumentation, which is stored in a file call \"utils\". \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL DEDICATED TO IMPORT PACKAGES USEFULL FOR OUR PURPOSES\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import os\n",
    "import cv2\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import History\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import plot_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "\n",
    "from keras.layers import Conv2D,Flatten,Dropout, MaxPooling2D,AveragePooling2D\n",
    "\n",
    "import import_ipynb\n",
    "import utils\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOME IMPORTANT GLOBAL VARIABLE\n",
    "labels = [\"Bedroom\",\"Coast\",\"Forest\",\"HighWay\",\"Industrial\",\"InsideCity\",\"Kitchen\",\"LivingRoom\",\"Mountain\",\"Office\",\"OpenCountry\",\"Store\",\"Street\",\"Suburb\",\"TallBuilding\"]\n",
    "train_dir = '../images/train/'\n",
    "test_dir = '../images/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creation of  initial data\n",
    "#train data\n",
    "list_of_images = utils.list_of_path(labels,train_dir)\n",
    "train_data,train_labels= utils.read_and_process_images(list_of_images)\n",
    "\n",
    "#test data\n",
    "list_of_images_test = utils.list_of_path(labels,test_dir)\n",
    "test_data,test_labels = utils.read_and_process_images(list_of_images_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA AUGMENTATION \n",
    "\n",
    "In order to try to achieve a better result, first of all we will try to augment our available data by performing a left-to-right tranformation and cropping. Details of how function \"data_augmentation\" works are in the presentation paper and in the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"augmented train data with left-to-right reflection and cropping\"\"\"\n",
    "train_data_aug,train_labels_aug = utils.data_augmentation(train_data,train_labels,cropping=True,crop_y=14,crop_x=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"reshape the data in order to make them feasible to cnn\"\"\"\n",
    "train_data_aug = train_data_aug.reshape(train_data_aug.shape[0],train_data_aug.shape[1],train_data_aug.shape[2],1)\n",
    "test_data = test_data.reshape(test_data.shape[0],test_data.shape[1],test_data.shape[2],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"create one-hot-encoding to make labels feasible for the cnn\"\"\"\n",
    "test_labels_dummy = to_categorical(test_labels,15)\n",
    "train_labels_dummy_aug = to_categorical(train_labels_aug,15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATION OF THE CNN \n",
    "\n",
    "FIrstly we will try to simply apply the same cnn that we build in the previus part of the exercise and see how the solution gets improved (remember: test_accuracy~0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(momentum=0.9,nesterov=True,decay = 1e-6, lr = 0.01)\n",
    "norm = initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#first convolutional layer\n",
    "model.add(Conv2D(filters=8,kernel_size=3,padding = 'valid',activation='relu',input_shape=(64,64,1)))\n",
    "model.add(MaxPooling2D(pool_size=2,strides=2))\n",
    "\n",
    "#secondo convolutional layer\n",
    "model.add(Conv2D(filters=16,kernel_size=3,strides=2,padding='valid',activation='relu',input_shape=(64,64,1)))\n",
    "model.add(MaxPooling2D(pool_size=2,strides=2))\n",
    "\n",
    "#third convolutional layer\n",
    "model.add(Conv2D(filters=32,kernel_size=3,padding='valid',activation='relu',input_shape=(64,64,1)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(15,activation='relu',kernel_initializer=norm,bias_initializer='zeros'))\n",
    "model.add(Dense(15,activation='softmax'))\n",
    "\n",
    "#classification output\n",
    "model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = History()\n",
    "earlyStopping = EarlyStopping(min_delta=0.10,patience = 10)\n",
    "\n",
    "start_time = time.time()\n",
    "model.fit(train_data_aug,train_labels_dummy_aug,batch_size=32,epochs=100,validation_split=0.20,shuffle=True,callbacks=[earlyStopping,history])\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_data,test_labels_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"time in seconds: \",end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = model.predict_proba(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOTTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"validation loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.grid()\n",
    "plt.savefig(\"images_point_two/val_loss_data_augmentation_only.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"validation accuracy\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.grid()\n",
    "plt.savefig(\"images_point_two/val_acc_data_augmentation_only.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\" accuracy\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.grid()\n",
    "plt.savefig(\"images_point_two/acc_data_augmentation_only.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.plot(history.history['loss'])\n",
    "plt.savefig(\"images_point_two/loss_data_augmentation_only.jpg\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "print(\"My Custom CNN Network:\")\n",
    "plot_model(model, to_file='Images_point_two/custom-cnn.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OBSERVATION \n",
    "\n",
    "Only with the augmentation of data, we reach a better results in terms of test accuracy (we pass fro 0.33 to 0.44): what to do now? We can try to perform:\n",
    "\n",
    "-BatchNormalization\n",
    "\n",
    "-Add Dropout\n",
    "\n",
    "-Change structure of the CNN, for example by introducing convolutional layer or by changing the size of filters\n",
    "\n",
    "\n",
    "Our aim is to reach a results better than 0.60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"I wrote a generic function in which I preserve the main structure of the cnn built in the \n",
    "first part of the exercise but I add the possibility to add two dropout layer (choosing also\n",
    "the dropout_rate) and 3 batch layer after performing relu \"\"\"\n",
    "adam = optimizers.Adam()\n",
    "def cnn(optim,kernel_sz,batch = False, Drop = False, Dropout_rate = 0):\n",
    "    \"\"\"\n",
    "    Function which creates a convolutional neural network\n",
    "    \n",
    "    Input optim = type of optimizer\n",
    "    Input Kernel_sz = size of the kernel\n",
    "    Input batch (False) = if True, it aplies batch normalization \n",
    "    Input Drop (False) = if True, it aplies dropout\n",
    "    Input Dropout_rate (0) = rate of the dropout\n",
    "    \n",
    "    Output = convolutional neural network \n",
    "    \"\"\"\n",
    "    np.random.seed(9)\n",
    "    model = Sequential()\n",
    "    #first convolutional layer\n",
    "    model.add(Conv2D(filters=8,kernel_size=kernel_sz,padding = 'valid',activation='relu',input_shape=(64,64,1)))\n",
    "    if(batch ==True):\n",
    "        model.add(BatchNormalization(axis = 1))\n",
    "    model.add(MaxPooling2D(pool_size=2,strides=2))\n",
    "    if(Drop==True):\n",
    "        model.add(Dropout(Dropout_rate))\n",
    "\n",
    "    #secondo convolutional layer\n",
    "    model.add(Conv2D(filters=16,kernel_size=kernel_sz,strides=2,padding='valid',activation='relu',input_shape=(64,64,1)))\n",
    "    if(batch==True):\n",
    "        model.add(BatchNormalization(axis = 1))\n",
    "    model.add(MaxPooling2D(pool_size=2,strides=2))\n",
    "    if(Drop==True):\n",
    "        model.add(Dropout(Dropout_rate))\n",
    "\n",
    "    #third convolutional layer\n",
    "    model.add(Conv2D(filters=32,kernel_size=kernel_sz,padding='valid',activation='relu',input_shape=(64,64,1)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(15,activation='relu',kernel_initializer=norm,bias_initializer='zeros'))\n",
    "    if(batch==True):\n",
    "        model.add(BatchNormalization(axis = 1))\n",
    "    model.add(Dense(15,activation='softmax'))\n",
    "\n",
    "    #classification output\n",
    "    model.compile(loss='categorical_crossentropy',optimizer= optim,metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def cnn_2():\n",
    "    \"\"\"\n",
    "    Function which creates a simple cnn with no particular input (used in ensemble of network)\n",
    "    \n",
    "    Input = None\n",
    "    \n",
    "    Output = cnn\n",
    "    \"\"\"\n",
    "    np.random.seed(9)\n",
    "    model = Sequential()\n",
    "    #first convolutional layer\n",
    "    model.add(Conv2D(filters=8,kernel_size=(3,3),padding = 'valid',activation='relu',input_shape=(64,64,1)))\n",
    "    model.add(BatchNormalization(axis = 1))\n",
    "    model.add(MaxPooling2D(pool_size=2,strides=2))\n",
    "    \n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    #secondo convolutional layer\n",
    "    model.add(Conv2D(filters=16,kernel_size=(3,3),strides=2,padding='valid',activation='relu',input_shape=(64,64,1)))\n",
    "    model.add(BatchNormalization(axis = 1))\n",
    "    model.add(MaxPooling2D(pool_size=2,strides=2))\n",
    "    \n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    #third convolutional layer\n",
    "    model.add(Conv2D(filters=32,kernel_size=(3,3),padding='valid',activation='relu',input_shape=(64,64,1)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(15,activation='relu',kernel_initializer=norm,bias_initializer='zeros'))\n",
    "    \n",
    "    model.add(BatchNormalization(axis = 1))\n",
    "    model.add(Dense(15,activation='softmax'))\n",
    "\n",
    "    #classification output\n",
    "    model.compile(loss='categorical_crossentropy',optimizer= adam,metrics = ['accuracy'])\n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"model_2: respect to the originale model, I just add dropout (with rate 0.5) and batchnormalization \"\"\"\n",
    "model_2 = cnn(adam,3,batch=True,Drop=True,Dropout_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_2 = History()\n",
    "earlyStopping_2 = EarlyStopping(min_delta=0.10,patience = 10)\n",
    "start_time_2 = time.time()\n",
    "model_2.fit(train_data_aug,train_labels_dummy_aug,batch_size=32,epochs=100,validation_split=0.20,shuffle=True,callbacks=[earlyStopping_2,history_2])\n",
    "end_time_2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluation of the model\"\"\"\n",
    "model_2.evaluate(test_data,test_labels_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"time in seconds: \",end_time_2-start_time_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"plotting the model\"\"\"\n",
    "plot_model(model_2, to_file='Images_point_two/cnn-dropout-batch.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cofunsion matrix of model with batch normalization and dropout \n",
    "plt.figure(figsize=(12,6)) \n",
    "y_pred = model.predict_classes(test_data)\n",
    "cm = confusion_matrix(test_labels,y_pred)\n",
    "utils.plot_confusion_matrix(cm,labels,\"point_two_dropout_batch_cm\",\"images_point_two/\",normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"validation loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"val-loss\")\n",
    "plt.plot(history_2.history['val_loss'])\n",
    "plt.grid()\n",
    "plt.savefig(\"images_point_two/validation_loss_dropout_batch.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"validation accuracy\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.plot(history_2.history['val_accuracy'])\n",
    "plt.grid()\n",
    "plt.savefig(\"images_point_two/validation_accuracy_dropout_batch.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\" accuracy\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.plot(history_2.history['accuracy'])\n",
    "plt.grid()\n",
    "plt.savefig(\"images_point_two/accuracy_dropout_batch.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.plot(history_2.history['loss'])\n",
    "plt.grid()\n",
    "plt.savefig(\"images_point_two/loss_dropout_batch.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"change only the size of the kernels--> structures are always the same as the first convolutional network \"\"\"\n",
    "model_3 = cnn(adam,5,batch=True,Drop=True,Dropout_rate=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history_3 = History()\n",
    "earlyStopping_3 = EarlyStopping(min_delta=0.10,patience = 10)\n",
    "start_time_3 = time.time()\n",
    "model_3.fit(train_data_aug,train_labels_dummy_aug,batch_size=32,epochs=100,validation_split=0.15,shuffle=True,callbacks=[earlyStopping_3,history_3])\n",
    "end_time_3 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.evaluate(test_data,test_labels_dummy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"time in seconds: \",end_time_3-start_time_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_3,show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENSEMBLE OF NETWORK \n",
    "\n",
    "In the following cells I wrote two functions in order to perform the so called \"ensemble of network\".\n",
    "\n",
    "-In the first function I train n times a cnn and I perform a prediction on test data in terms of probabilistic distribution along the 15 different classes.\n",
    "\n",
    "-In the second class I perform the mean along the different obtained results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ensemble_of_network(mod,iteration,train_data, train_labels,test_data):\n",
    "    \"\"\"\n",
    "    Function which performs ensemple of cnn model of the same type\n",
    "    \n",
    "    Input mod = function which gives a specific cnn\n",
    "    Input iteration = number of model to be \"ensembled\"\n",
    "    train_data = data for training the cnn\n",
    "    train_labels = labels of the train data\n",
    "    test_data = data to test and evaluate cnn\n",
    "    \n",
    "    Output prediction = array with prediction of test data in terms of probability, obtained with \n",
    "                        average\n",
    "    Output pred_voting = array with prediction  \n",
    "    Output validation_acc = all validation accuracy arrays of all models\n",
    "    Output acc = all  accuracy arrays of all models\n",
    "    Output validation_loss = all  validation_loss arrays of all models\n",
    "    Output loss = all  loss arrays of all models\n",
    "    \"\"\"\n",
    "    history = History()\n",
    "    earlyStopping = EarlyStopping(min_delta=0.0,patience = 10)\n",
    "    prediction = np.zeros((test_data.shape[0],15))\n",
    "    loss = []\n",
    "    validation_loss = []\n",
    "    validation_acc = []\n",
    "    acc = []\n",
    "    pred_voting = []\n",
    "    for i in range(iteration):\n",
    "        model_temp = mod()\n",
    "        model_temp.fit(train_data,train_labels,batch_size=32,epochs=100,validation_split=0.15,shuffle=True,callbacks=[earlyStopping,history])\n",
    "        prediction = prediction + model_temp.predict_proba(test_data)\n",
    "        pred_voting.append(model_temp.predict_classes(test_data))\n",
    "        loss.append(history.history['loss'])\n",
    "        validation_loss.append(history.history['val_loss'])\n",
    "        validation_acc.append(history.history['val_accuracy'])\n",
    "        acc.append(history.history['accuracy'])\n",
    "    \n",
    "    prediction = prediction/iteration\n",
    "    \n",
    "    return prediction,pred_voting,validation_acc,acc,validation_loss,loss\n",
    "    \n",
    "        \n",
    "def calculate_accuracy(prediction,test_labels):\n",
    "    \"\"\"\n",
    "    Function which calculate accuracy of the ensemble of network in case of average solution\n",
    "    \n",
    "    Input prediction = array with prediction of test data in terms of probability, obtained with \n",
    "                       average\n",
    "    test_labels = array with true labels of test data\n",
    "    \n",
    "    Output res = accuracy \n",
    "    \"\"\"\n",
    "    total = test_labels.shape[0]\n",
    "    cont = 0\n",
    "    for i in range(len(prediction)):\n",
    "        if(np.argmax(prediction[i])==test_labels[i]):\n",
    "            cont = cont + 1\n",
    "        else: \n",
    "            \n",
    "            \n",
    "            continue \n",
    "    res = cont/total\n",
    "    return res \n",
    "\n",
    "##################################################################################\n",
    "def determ_prediction(pred,test_lab):\n",
    "    \"\"\"\n",
    "    function which determs final prediction with voting\n",
    "    \n",
    "    Input pred = predictions of ensemble of network\n",
    "    Input test_data = test dataset\n",
    "    \n",
    "    Output res = array of final prediction for test data\n",
    "    \"\"\"\n",
    "    res = np.zeros(len(pred[0]))\n",
    "    for i in range(len(pred[0])):\n",
    "        cont = np.zeros(15)\n",
    "        #viaggio per le immagini\n",
    "        for j in range(len(pred)):\n",
    "            #viaggio per i tentativi di cnn\n",
    "            \n",
    "            cont[pred[j][i]]+=1\n",
    "        \n",
    "        maxi = np.argmax(cont)\n",
    "        res[i] = maxi\n",
    "        \n",
    "    return res\n",
    "    \n",
    "\n",
    "def calulate_accuracy_with_voting(pred,test_labels):\n",
    "    \"\"\"\n",
    "    Function which calculate accuracy if we apply voting\n",
    "    \n",
    "    Input pred = array of prediction \n",
    "    Input test_labels = array with true labels of test data\n",
    "    \n",
    "    Output res = accuracy\n",
    "    \"\"\"\n",
    "    total = test_labels.shape[0]\n",
    "    cont = 0\n",
    "    for i in range(len(pred)):\n",
    "        if(pred[i]==test_labels[i]):\n",
    "            cont = cont +1\n",
    "        else:\n",
    "            continue\n",
    "    res = cont/total\n",
    "    return res\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Apply the algorithm which preforms the ensemble of network\"\"\"\n",
    "ensembel_start_time = time.time()\n",
    "pred,pred_vot,val_acc,val,val_loss,loss = ensemble_of_network(cnn_2,10,train_data_aug,train_labels_dummy_aug,test_data)\n",
    "ensemble_end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_accuracy(pred,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_ensemble = convert(ensemble_end_time-ensembel_start_time)\n",
    "time_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"validation accuracy\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "for p in val_acc:\n",
    "    plt.plot(p,linewidth=0.5)\n",
    "\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"accuracy\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "for p in val:\n",
    "    plt.plot(p,linewidth=0.5)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"validation loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"val-loss\")\n",
    "for p in val_loss:\n",
    "    plt.plot(p,linewidth=0.5)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "for p in loss:\n",
    "    plt.plot(p,linewidth=0.5)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = determ_prediction(pred_vot,test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = calulate_accuracy_with_voting(res,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# play with number of layers\n",
    "\n",
    "In this last part I simply do some \"random\" experiment, by adding some layer, both convolutional and dense, to my cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(9)\n",
    "model_4 = Sequential()\n",
    "#first convolutional layer\n",
    "model_4.add(Conv2D(filters=8,kernel_size=(3,3),strides = 1,padding = 'valid',activation='relu',input_shape=(64,64,1)))\n",
    "model_4.add(BatchNormalization(axis=1))\n",
    "model_4.add(MaxPooling2D(pool_size=2,strides=2))\n",
    "model_4.add(Dropout(0.3))\n",
    "\n",
    "#secondo convolutional layer\n",
    "model_4.add(Conv2D(filters=16,kernel_size=(3,3),strides=1,padding='valid',activation='relu',input_shape=(64,64,1)))\n",
    "model_4.add(BatchNormalization())\n",
    "model_4.add(Dropout(0.3))\n",
    "#third convolutional layer\n",
    "model_4.add(Conv2D(filters=32,kernel_size=(3,3),padding='valid',activation='relu',input_shape=(64,64,1)))\n",
    "model_4.add(BatchNormalization())\n",
    "model_4.add(Dropout(0.3))\n",
    "\n",
    "#add 1 convolutional layer\n",
    "model_4.add(Conv2D(filters=64,kernel_size=(3,3),padding='valid',activation='relu',input_shape=(64,64,1)))\n",
    "model_4.add(BatchNormalization())\n",
    "model_4.add(MaxPooling2D(pool_size=2,strides=2))\n",
    "model_4.add(Dropout(0.3))\n",
    "\n",
    "\n",
    "model_4.add(Flatten())\n",
    "#add 1 dense layer\n",
    "\n",
    "model_4.add(Dense(30,activation='relu',kernel_initializer=norm,bias_initializer='zeros'))\n",
    "model_4.add(BatchNormalization())\n",
    "model_4.add(Dense(15,activation='relu'))\n",
    "model_4.add(BatchNormalization())\n",
    "model_4.add(Dense(15,activation='softmax'))\n",
    "\n",
    "#classification output\n",
    "model_4.compile(loss='categorical_crossentropy',optimizer= adam,metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_4,show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_4 = History()\n",
    "earlyStopping_4 = EarlyStopping(min_delta=0.10,patience = 10)\n",
    "start_time_4 = time.time()\n",
    "model_4.fit(train_data_aug,train_labels_dummy_aug,batch_size=32,epochs=100,validation_split=0.15,shuffle=True,callbacks=[earlyStopping_4,history_4])\n",
    "end_time_4 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4.evaluate(test_data,test_labels_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_4 = convert(end_time_4-start_time_4)\n",
    "time_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "print(\"My Custom CNN Network:\")\n",
    "plot_model(model_4, to_file='Images_point_two/deeper_model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
