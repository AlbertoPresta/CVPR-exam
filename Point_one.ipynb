{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIRST ATTEMPT TO CNN\n",
    "\n",
    "In this first file , I will complete the first banch of points requested by the project. In particular, the main goal is to build a shallow convolutional neural network able to classify images from 15 different classes with a test accuracy of at least 30%.\n",
    "\n",
    "Before doing so, I have to preprocess my data. Indeed what I have is a group of black and white images of 15 different categories, split in train set and in test set. I wrote 2 functions (saved in another script file called utils) in which I collect all the images's directories and I save all of them as numpy matrices (Morevore I performed a resize of each images in order to have a (64,64,1)-dimension images).\n",
    "\n",
    "The details of these functions are described in the presentation and in the script file dedicated to them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IN this cell I simply upload the packages I used to build the cnn or to do some other operation (read directorie, plot something, do mathematical operation etc...) \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import os\n",
    "import cv2\n",
    "from keras import initializers\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical #to create dummy variable\n",
    "from keras.layers import Conv2D,Flatten,Dropout, MaxPooling2D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import History\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# In this part I simply importe the other script file I wrote in which there are some useful function to process data\n",
    "import import_ipynb\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESSING DEI  DATI \n",
    "\n",
    "In the following cells I simply construct and prepare my data in order to make them feasible for my cnn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOME IMPORTANT GLOBAL VARIABLE\n",
    "base_dir = '../images/'\n",
    "train_dir=os.path.join(base_dir,'train/')\n",
    "test_dir=os.path.join(base_dir,'test/')\n",
    "labels = [\"Bedroom\",\"Coast\",\"Forest\",\"HighWay\",\"Industrial\",\"InsideCity\",\"Kitchen\",\"LivingRoom\",\"Mountain\",\"Office\",\"OpenCountry\",\"Store\",\"Street\",\"Suburb\",\"TallBuilding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "                                PREPARING MY TRAIN DATA\n",
    "What I obtain at the end of these 2 functions is a 3-dimensional numpy-ndarray in which \n",
    "there are saved 1500 normalized images of dimension 64x64, with their respective labels. \n",
    "Morevorer I shuffle the images in order not to have the initial order (this is necessary \n",
    "when I split data in train set and validation set).\n",
    "\"\"\"\n",
    "list_of_images = utils.list_of_path(labels,train_dir)\n",
    "train_data,train_labels= utils.read_and_process_images(list_of_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "I perform the same as the previous cell in order to create test data\n",
    "\"\"\"\n",
    "list_of_images_test = utils.list_of_path(labels,test_dir)\n",
    "test_data,test_labels = utils.read_and_process_images(list_of_images_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this cell I reshape my data. In particular I add a dimension at the end in order to express\n",
    "the number of channel of each image. This reshape is necessary, otherwise I would have \n",
    "an error when I train the cnn. \n",
    "\"\"\"\n",
    "train_data = train_data.reshape(train_data.shape[0],train_data.shape[1],train_data.shape[2],1)\n",
    "test_data = test_data.reshape(test_data.shape[0],test_data.shape[1],test_data.shape[2],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_dummy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_dummy = to_categorical(train_labels,15)\n",
    "test_labels_dummy = to_categorical(test_labels,15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINITION OF THE MODEL \n",
    "\n",
    "In order to define the cnn I wrote this first function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_cnn():\n",
    "    \"\"\"\n",
    "    This function represents the first convolutional neural network requested to the projects\n",
    "    It doesn't receive any input.\n",
    "    As optimizer it uses stochastic gradient descend with nesterov momentum and it is formed by 3 convolutional layer \n",
    "    plus a dense layer with relu as activation function\n",
    "    At the end there is softmax, as we are dealing with classification problem\n",
    "    \n",
    "    Input = None\n",
    "    \n",
    "    Output = not trained model \n",
    "    \"\"\"\n",
    "    np.random.seed(9)\n",
    "    sgd = optimizers.SGD(momentum=0.9,nesterov=True)\n",
    "    norm = initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None)\n",
    "    model = Sequential()\n",
    "    #first convolutional layer\n",
    "    model.add(Conv2D(filters=8,kernel_size=3,padding = 'valid',activation='relu',input_shape=(64,64,1)))\n",
    "    model.add(MaxPooling2D(pool_size=2,strides=2))\n",
    "\n",
    "    #secondo convolutional layer\n",
    "    model.add(Conv2D(filters=16,kernel_size=3,strides=2,padding='valid',activation='relu',input_shape=(64,64,1)))\n",
    "    model.add(MaxPooling2D(pool_size=2,strides=2))\n",
    "\n",
    "    #third convolutional layer\n",
    "    model.add(Conv2D(filters=32,kernel_size=3,padding='valid',activation='relu',input_shape=(64,64,1)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(15,activation='relu',kernel_initializer=norm,bias_initializer='zeros'))\n",
    "    model.add(Dense(15,activation='softmax'))\n",
    "\n",
    "    #classification output\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL TRAINING AND TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "IN THIS CELL I TRAIN MY CNN.\n",
    "To do so, I first define history (in order to collect values of accuracy and loss) and \n",
    "earlystopping in order to stop the training before the end of epochs if it is necessary.\n",
    "\"\"\"\n",
    "\n",
    "history = History()\n",
    "earlyStopping = EarlyStopping(min_delta=0.10,patience = 10)\n",
    "model = first_cnn()\n",
    "model.fit(train_data,train_labels_dummy,batch_size=32,epochs=100,validation_split=0.15,shuffle=True,callbacks=[earlyStopping,history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "I evaluate the model in the test set\n",
    "\"\"\"\n",
    "model.evaluate(test_data,test_labels_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "I predict classes of my test set\n",
    "\"\"\"\n",
    "y_pred = model.predict_classes(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"obtaining the confusion matrix\"\"\"\n",
    "cm = confusion_matrix(test_labels,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOTTING \n",
    "\n",
    "In the remaining cells I perform some plots that can be interested to comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "utils.plot_confusion_matrix(cm,labels,\"point_one\",\"images_point_one/\",normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"validation loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"val-loss\")\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.grid()\n",
    "plt.savefig(\"images_point_one/validation_loss.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.title(\"validation accuracy\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.grid()\n",
    "plt.savefig(\"images_point_one/validation_accuracy.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\" accuracy\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.grid()\n",
    "plt.savefig(\"images_point_one/accuracy.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.plot(history.history['loss'])\n",
    "plt.grid()\n",
    "plt.savefig(\"images_point_one/loss.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECOND PHASE -->TRY TO IMPROVE MY RESULTS\n",
    "\n",
    "Now, whit this cnn, I reach a test accuracy of ~35%, but I would like to improve this results. In the next script, called \"Point_two\", I will exploit some characteristics and features of the cnn to try to get a test accuracy of at least 60%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
